{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e857aab2-adf2-46d5-8648-5ca819637f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, avg, sum as spark_sum, max as spark_max, min as spark_min,\n",
    "    desc, when, year, month, dayofmonth, round, lit, datediff, current_date,\n",
    "    countDistinct\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "675a5762-dccd-4aa5-9fc9-cb213e352482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Daily Market Prices of Commodity in India from 2022 to 2025...\nData loaded successfully\n   • Total rows: 20,090,620\n   • Columns: 11\n\nSchema:\nroot\n |-- State: string (nullable = true)\n |-- District: string (nullable = true)\n |-- Market: string (nullable = true)\n |-- Commodity: string (nullable = true)\n |-- Variety: string (nullable = true)\n |-- Grade: string (nullable = true)\n |-- Arrival_Date: date (nullable = true)\n |-- Min_Price: double (nullable = true)\n |-- Max_Price: double (nullable = true)\n |-- Modal_Price: double (nullable = true)\n |-- Commodity_Code: long (nullable = true)\n\n\nFirst 10 rows:\n+--------------+------------+--------------------+--------------+-------------+-----+------------+---------+---------+-----------+--------------+\n|         State|    District|              Market|     Commodity|      Variety|Grade|Arrival_Date|Min_Price|Max_Price|Modal_Price|Commodity_Code|\n+--------------+------------+--------------------+--------------+-------------+-----+------------+---------+---------+-----------+--------------+\n|Andhra Pradesh|     Chittor|            Chittoor| Gur (Jaggery)|         NO 2|  FAQ|  2025-01-01|   3200.0|   3500.0|     3500.0|            74|\n|    Tamil Nadu|The Nilgiris|Gudalur (Uzhavar ...|        Garlic|      Average|Local|  2025-01-01|  28000.0|  30000.0|    30000.0|            25|\n|    Tamil Nadu|The Nilgiris|Gudalur (Uzhavar ...|Ginger (Green)| Green Ginger|Local|  2025-01-01|   7000.0|   8000.0|     8000.0|           103|\n|    Tamil Nadu|The Nilgiris|Gudalur (Uzhavar ...|  Green Chilli| Green Chilly|Local|  2025-01-01|   4500.0|   5000.0|     5000.0|            87|\n|    Tamil Nadu|The Nilgiris|Gudalur (Uzhavar ...| Mint (Pudina)|Mint (Pudina)|Local|  2025-01-01|   4500.0|   5000.0|     5000.0|           360|\n|    Tamil Nadu|The Nilgiris|Gudalur (Uzhavar ...|         Onion|      Bellary|Local|  2025-01-01|   4200.0|   4400.0|     4400.0|            23|\n|    Tamil Nadu|The Nilgiris|Gudalur (Uzhavar ...|   Onion Green|  Onion Green|Local|  2025-01-01|   7000.0|   7400.0|     7400.0|           358|\n|    Tamil Nadu|The Nilgiris|Gudalur (Uzhavar ...|        Potato|(Red Nanital)|Local|  2025-01-01|   4500.0|   5000.0|     5000.0|            24|\n|    Tamil Nadu|The Nilgiris|Gudalur (Uzhavar ...|       Pumpkin|      Pumpkin|Local|  2025-01-01|   1800.0|   2000.0|     2000.0|            84|\n|    Tamil Nadu|The Nilgiris|Gudalur (Uzhavar ...|       Raddish|      Raddish|Local|  2025-01-01|   3000.0|   3500.0|     3500.0|           161|\n+--------------+------------+--------------------+--------------+-------------+-----+------------+---------+---------+-----------+--------------+\nonly showing top 10 rows\nColumn Information:\n    1. State                → string\n    2. District             → string\n    3. Market               → string\n    4. Commodity            → string\n    5. Variety              → string\n    6. Grade                → string\n    7. Arrival_Date         → date\n    8. Min_Price            → double\n    9. Max_Price            → double\n   10. Modal_Price          → double\n   11. Commodity_Code       → bigint\n\nMissing values:\n+-----+--------+------+---------+-------+-----+------------+---------+---------+-----------+--------------+\n|State|District|Market|Commodity|Variety|Grade|Arrival_Date|Min_Price|Max_Price|Modal_Price|Commodity_Code|\n+-----+--------+------+---------+-------+-----+------------+---------+---------+-----------+--------------+\n|    0|       0|     0|        0|      0|    0|           0|        0|        0|          0|             0|\n+-----+--------+------+---------+-------+-----+------------+---------+---------+-----------+--------------+\n\n\nDuplicate Check:\n   • Total rows: 20,090,620\n   • Unique rows: 20,090,587\n   • Duplicate rows: 33\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# loading data\n",
    "\n",
    "print(\"Loading Daily Market Prices of Commodity in India from 2022 to 2025...\")\n",
    "\n",
    "df = spark.table(\"commodities_india\")\n",
    "\n",
    "print(f\"Data loaded successfully\")\n",
    "print(f\"   • Total rows: {df.count():,}\")\n",
    "print(f\"   • Columns: {len(df.columns)}\")\n",
    "print()\n",
    "\n",
    "# show schema\n",
    "print(\"Schema:\")\n",
    "df.printSchema()\n",
    "print()\n",
    "\n",
    "# preview data\n",
    "print(\"First 10 rows:\")\n",
    "df.show(10)\n",
    "\n",
    "# show column names and types\n",
    "print(\"Column Information:\")\n",
    "for i, (col_name, dtype) in enumerate(df.dtypes, 1):\n",
    "    print(f\"   {i:2d}. {col_name:20s} → {dtype}\")\n",
    "print()\n",
    "\n",
    "\n",
    "# show missing values\n",
    "print(\"Missing values:\")\n",
    "null_counts = df.select([\n",
    "    spark_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) \n",
    "    for c in df.columns\n",
    "])\n",
    "null_counts.show()\n",
    "print()\n",
    "\n",
    "# show any duplicates\n",
    "total_rows = df.count()\n",
    "unique_rows = df.dropDuplicates().count()\n",
    "duplicate_rows = total_rows - unique_rows\n",
    "\n",
    "print(f\"Duplicate Check:\")\n",
    "print(f\"   • Total rows: {total_rows:,}\")\n",
    "print(f\"   • Unique rows: {unique_rows:,}\")\n",
    "print(f\"   • Duplicate rows: {duplicate_rows:,}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ceff014-b945-44ba-9a28-3b515e4e4850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILTER 1: REMOVING INVALID PRICES\nFilter applied: Remove invalid prices\n   • Rows removed: 30,869\n   • Rows remaining: 20,059,751\n\nFILTER 2: REMOVING DUPLICATE ROWS\nBefore removing duplicates:\n   • Total rows: 20,059,751\n   • Unique rows: 20,059,718\n   • Duplicate rows: 33\n\nFilter applied: Remove duplicate rows\n   • Rows removed: 33\n   • Rows remaining: 20,059,718\n\n"
     ]
    }
   ],
   "source": [
    "# filter 1\n",
    "\n",
    "print(\"FILTER 1: REMOVING INVALID PRICES\")\n",
    "\n",
    "# Filter out rows where prices are null or negative\n",
    "df_filtered = df.filter(\n",
    "    (col(\"Min_Price\").isNotNull()) &\n",
    "    (col(\"Max_Price\").isNotNull()) &\n",
    "    (col(\"Modal_Price\").isNotNull()) &\n",
    "    (col(\"Min_Price\") > 0) &\n",
    "    (col(\"Max_Price\") > 0) &\n",
    "    (col(\"Modal_Price\") > 0)\n",
    ")\n",
    "\n",
    "rows_after_filter1 = df_filtered.count()\n",
    "rows_removed = df.count() - rows_after_filter1\n",
    "\n",
    "print(f\"Filter applied: Remove invalid prices\")\n",
    "print(f\"   • Rows removed: {rows_removed:,}\")\n",
    "print(f\"   • Rows remaining: {rows_after_filter1:,}\")\n",
    "print()\n",
    "\n",
    "# filter 2\n",
    "\n",
    "print(\"FILTER 2: REMOVING DUPLICATE ROWS\")\n",
    "\n",
    "# checking first\n",
    "total_before = df_filtered.count()\n",
    "unique_before = df_filtered.dropDuplicates().count()\n",
    "duplicates = total_before - unique_before\n",
    "\n",
    "print(f\"Before removing duplicates:\")\n",
    "print(f\"   • Total rows: {total_before:,}\")\n",
    "print(f\"   • Unique rows: {unique_before:,}\")\n",
    "print(f\"   • Duplicate rows: {duplicates:,}\")\n",
    "print()\n",
    "\n",
    "df_filtered = df_filtered.dropDuplicates()\n",
    "\n",
    "rows_after_filter2 = df_filtered.count()\n",
    "\n",
    "print(f\"Filter applied: Remove duplicate rows\")\n",
    "print(f\"   • Rows removed: {duplicates:,}\")\n",
    "print(f\"   • Rows remaining: {rows_after_filter2:,}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a2e0a4-855c-4035-b402-dc394f31872f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOIN OPERATION: Mapping States to Regions\nRegions in lookup table:\n+---------+-------------------+\n|Region   |Region_Full_Name   |\n+---------+-------------------+\n|Central  |Central Region     |\n|East     |Eastern Region     |\n|North    |Northern Region    |\n|Northeast|Northeastern Region|\n|South    |Southern Region    |\n|West     |Western Region     |\n+---------+-------------------+\n\n\nLEFT JOIN\nJoining main data with region lookup...\nJoin complete\n   • Records before join: 20,059,718\n   • Records after join: 20,059,718\n\nAll records matched\n\nSample data with region:\n+--------------+-------+------------+----------------------+-----------+------------+\n|State         |Region |District    |Commodity             |Modal_Price|Arrival_Date|\n+--------------+-------+------------+----------------------+-----------+------------+\n|Maharashtra   |West   |Satara      |Tomato                |2000.0     |2023-08-27  |\n|Rajasthan     |West   |Hanumangarh |Onion                 |2400.0     |2023-08-27  |\n|Uttar Pradesh |North  |Basti       |Bitter gourd          |2640.0     |2023-08-27  |\n|Maharashtra   |West   |Ahmednagar  |Cabbage               |1000.0     |2023-08-27  |\n|NCT of Delhi  |North  |Delhi       |Little gourd (Kundru) |1600.0     |2023-08-27  |\n|Punjab        |North  |kapurthala  |Colacasia             |1500.0     |2023-08-27  |\n|Telangana     |South  |Nalgonda    |Bhindi (Ladies Finger)|3000.0     |2023-08-27  |\n|Uttar Pradesh |North  |Basti       |Rice                  |2780.0     |2023-08-27  |\n|Uttar Pradesh |North  |Bulandshahar|Potato                |1235.0     |2023-08-27  |\n|Madhya Pradesh|Central|Katni       |Guava                 |500.0      |2023-08-27  |\n|Madhya Pradesh|Central|Rajgarh     |Lentil (Masur)(Whole) |6300.0     |2023-08-27  |\n|Telangana     |South  |Hyderabad   |Plum                  |2550.0     |2023-08-27  |\n|Uttar Pradesh |North  |Kaushambi   |Tomato                |3060.0     |2023-08-27  |\n|Uttar Pradesh |North  |Saharanpur  |Lemon                 |3050.0     |2023-08-27  |\n|Maharashtra   |West   |Pune        |Potato                |1500.0     |2023-08-27  |\n+--------------+-------+------------+----------------------+-----------+------------+\nonly showing top 15 rows\n\nJOIN VERIFICATION\nRecords by region:\n+---------+-------------------+-------------+----------+\n|Region   |Region_Full_Name   |total_records|num_states|\n+---------+-------------------+-------------+----------+\n|North    |Northern Region    |7070974      |8         |\n|South    |Southern Region    |6404696      |7         |\n|West     |Western Region     |3221756      |4         |\n|Central  |Central Region     |1660160      |2         |\n|East     |Eastern Region     |1268814      |4         |\n|Northeast|Northeastern Region|433318       |6         |\n+---------+-------------------+-------------+----------+\n\n\nAGGREGATION AFTER JOIN: REGIONAL ANALYSIS\nRegional price comparison:\n+---------+-------------------+-------------+----------+------------------+--------------+---------------+---------+------------+-------------+-------------+\n|Region   |Region_Full_Name   |total_records|num_states|unique_commodities|unique_markets|avg_modal_price|min_price|max_price   |avg_min_price|avg_max_price|\n+---------+-------------------+-------------+----------+------------------+--------------+---------------+---------+------------+-------------+-------------+\n|Northeast|Northeastern Region|433318       |6         |120               |143           |5860.6         |25.0     |700000.0    |5477.27      |6236.94      |\n|South    |Southern Region    |6404696      |7         |230               |1104          |5566.43        |0.07     |3950000.0   |5008.14      |5781.23      |\n|West     |Western Region     |3221756      |4         |200               |798           |5062.03        |1.0      |9.18421086E8|4438.03      |5656.31      |\n|East     |Eastern Region     |1268814      |4         |156               |265           |3734.47        |2.0      |600000.0    |3502.1       |3985.57      |\n|Central  |Central Region     |1660160      |2         |212               |528           |3528.22        |1.0      |4000000.0   |3052.93      |3827.72      |\n|North    |Northern Region    |7070974      |8         |216               |665           |3348.49        |0.1      |1300000.0   |3113.06      |3584.29      |\n+---------+-------------------+-------------+----------+------------------+--------------+---------------+---------+------------+-------------+-------------+\n\n\n"
     ]
    }
   ],
   "source": [
    "# join operation using regional data\n",
    "\n",
    "print(\"JOIN OPERATION: Mapping States to Regions\")\n",
    "\n",
    "# create a state region lookup table\n",
    "# group states in India by geographic region\n",
    "state_region_data = [\n",
    "    # South\n",
    "    (\"Andhra Pradesh\", \"South\", \"Southern Region\"),\n",
    "    (\"Telangana\", \"South\", \"Southern Region\"),\n",
    "    (\"Karnataka\", \"South\", \"Southern Region\"),\n",
    "    (\"Kerala\", \"South\", \"Southern Region\"),\n",
    "    (\"Tamil Nadu\", \"South\", \"Southern Region\"),\n",
    "    (\"Puducherry\", \"South\", \"Southern Region\"),\n",
    "    (\"Pondicherry\", \"South\", \"Southern Region\"),\n",
    "    (\"Andaman and Nicobar\", \"South\", \"Southern Region\"),\n",
    "    \n",
    "    # West\n",
    "    (\"Maharashtra\", \"West\", \"Western Region\"),\n",
    "    (\"Gujarat\", \"West\", \"Western Region\"),\n",
    "    (\"Goa\", \"West\", \"Western Region\"),\n",
    "    (\"Rajasthan\", \"West\", \"Western Region\"),\n",
    "    (\"Daman and Diu\", \"West\", \"Western Region\"),\n",
    "    (\"Dadra and Nagar Haveli\", \"West\", \"Western Region\"),\n",
    "    \n",
    "    # North\n",
    "    (\"Uttar Pradesh\", \"North\", \"Northern Region\"),\n",
    "    (\"Punjab\", \"North\", \"Northern Region\"),\n",
    "    (\"Haryana\", \"North\", \"Northern Region\"),\n",
    "    (\"Delhi\", \"North\", \"Northern Region\"),\n",
    "    (\"NCT of Delhi\", \"North\", \"Northern Region\"), \n",
    "    (\"Himachal Pradesh\", \"North\", \"Northern Region\"),\n",
    "    (\"Uttarakhand\", \"North\", \"Northern Region\"),\n",
    "    (\"Uttrakhand\", \"North\", \"Northern Region\"),\n",
    "    (\"Jammu and Kashmir\", \"North\", \"Northern Region\"),\n",
    "    (\"Chandigarh\", \"North\", \"Northern Region\"),\n",
    "    \n",
    "    # East\n",
    "    (\"West Bengal\", \"East\", \"Eastern Region\"),\n",
    "    (\"Bihar\", \"East\", \"Eastern Region\"),\n",
    "    (\"Jharkhand\", \"East\", \"Eastern Region\"),\n",
    "    (\"Odisha\", \"East\", \"Eastern Region\"),\n",
    "    \n",
    "    # Northeast\n",
    "    (\"Assam\", \"Northeast\", \"Northeastern Region\"),\n",
    "    (\"Manipur\", \"Northeast\", \"Northeastern Region\"),\n",
    "    (\"Meghalaya\", \"Northeast\", \"Northeastern Region\"),\n",
    "    (\"Tripura\", \"Northeast\", \"Northeastern Region\"),\n",
    "    (\"Nagaland\", \"Northeast\", \"Northeastern Region\"),\n",
    "    (\"Mizoram\", \"Northeast\", \"Northeastern Region\"),\n",
    "    (\"Arunachal Pradesh\", \"Northeast\", \"Northeastern Region\"),\n",
    "    (\"Sikkim\", \"Northeast\", \"Northeastern Region\"),\n",
    "    \n",
    "    # Central\n",
    "    (\"Madhya Pradesh\", \"Central\", \"Central Region\"),\n",
    "    (\"Chhattisgarh\", \"Central\", \"Central Region\"),\n",
    "    (\"Chattisgarh\", \"Central\", \"Central Region\")\n",
    "]\n",
    "\n",
    "# creating dataframe\n",
    "state_region_df = spark.createDataFrame(\n",
    "    state_region_data, \n",
    "    [\"State\", \"Region\", \"Region_Full_Name\"]\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Regions in lookup table:\")\n",
    "state_region_df.select(\"Region\", \"Region_Full_Name\").distinct().orderBy(\"Region\").show(truncate=False)\n",
    "print()\n",
    "\n",
    "# perform join operation\n",
    "print(\"LEFT JOIN\")\n",
    "\n",
    "print(\"Joining main data with region lookup...\")\n",
    "df_with_region = df_filtered.join(\n",
    "    state_region_df,\n",
    "    on=\"State\",\n",
    "    how=\"left\" \n",
    ")\n",
    "\n",
    "print(f\"Join complete\")\n",
    "print(f\"   • Records before join: {df_filtered.count():,}\")\n",
    "print(f\"   • Records after join: {df_with_region.count():,}\")\n",
    "print()\n",
    "\n",
    "# checking for unmatched states just in case \n",
    "unmatched = df_with_region.filter(col(\"Region\").isNull())\n",
    "unmatched_count = unmatched.count()\n",
    "\n",
    "if unmatched_count > 0:\n",
    "    print(f\"Warning: {unmatched_count:,} records didn't match (Region is NULL)\")\n",
    "    print(\"   States without region mapping:\")\n",
    "    unmatched.select(\"State\").distinct().show(10, truncate=False)\n",
    "    print()\n",
    "else:\n",
    "    print(\"All records matched\")\n",
    "    print()\n",
    "\n",
    "# sample of joined data\n",
    "print(\"Sample data with region:\")\n",
    "df_with_region.select(\n",
    "    \"State\", \"Region\", \"District\", \"Commodity\", \n",
    "    \"Modal_Price\", \"Arrival_Date\"\n",
    ").show(15, truncate=False)\n",
    "print()\n",
    "\n",
    "\n",
    "# look at join results\n",
    "\n",
    "print(\"JOIN VERIFICATION\")\n",
    "\n",
    "# counting records by region\n",
    "print(\"Records by region:\")\n",
    "region_counts = df_with_region \\\n",
    "    .filter(col(\"Region\").isNotNull()) \\\n",
    "    .groupBy(\"Region\", \"Region_Full_Name\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_records\"),\n",
    "        countDistinct(\"State\").alias(\"num_states\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_records\"))\n",
    "\n",
    "region_counts.show(truncate=False)\n",
    "print()\n",
    "\n",
    "# aggregate operation: Group by region\n",
    "\n",
    "print(\"AGGREGATION AFTER JOIN: REGIONAL ANALYSIS\")\n",
    "\n",
    "regional_stats = df_with_region \\\n",
    "    .filter(col(\"Region\").isNotNull()) \\\n",
    "    .groupBy(\"Region\", \"Region_Full_Name\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_records\"),\n",
    "        countDistinct(\"State\").alias(\"num_states\"),           \n",
    "        countDistinct(\"Commodity\").alias(\"unique_commodities\"), \n",
    "        countDistinct(\"Market\").alias(\"unique_markets\"), \n",
    "        round(avg(\"Modal_Price\"), 2).alias(\"avg_modal_price\"),\n",
    "        round(spark_min(\"Modal_Price\"), 2).alias(\"min_price\"),\n",
    "        round(spark_max(\"Modal_Price\"), 2).alias(\"max_price\"),\n",
    "        round(avg(\"Min_Price\"), 2).alias(\"avg_min_price\"),\n",
    "        round(avg(\"Max_Price\"), 2).alias(\"avg_max_price\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"avg_modal_price\"))\n",
    "\n",
    "print(\"Regional price comparison:\")\n",
    "regional_stats.show(truncate=False)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b5e1f5a-4b4d-44f4-ae41-2371130528e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------+-----------+---------+--------------------+\n|           Commodity|Min_Price|Max_Price|Modal_Price|Avg_Price|Price_Volatility_Pct|\n+--------------------+---------+---------+-----------+---------+--------------------+\n|              Tomato|   1000.0|   2000.0|     2000.0|   1500.0|                50.0|\n|               Onion|   2400.0|   2400.0|     2400.0|   2400.0|                 0.0|\n|        Bitter gourd|   2590.0|   2690.0|     2640.0|   2640.0|                3.79|\n|             Cabbage|    400.0|   1800.0|     1000.0|   1100.0|               140.0|\n|Little gourd (Kun...|   1400.0|   1800.0|     1600.0|   1600.0|                25.0|\n|           Colacasia|   1200.0|   1500.0|     1500.0|   1350.0|                20.0|\n|Bhindi (Ladies Fi...|   3000.0|   3000.0|     3000.0|   3000.0|                 0.0|\n|                Rice|   2730.0|   2830.0|     2780.0|   2780.0|                 3.6|\n|              Potato|   1130.0|   1340.0|     1235.0|   1235.0|                17.0|\n|               Guava|    500.0|    500.0|      500.0|    500.0|                 0.0|\n+--------------------+---------+---------+-----------+---------+--------------------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# column transformations using withColumn\n",
    "\n",
    "df_transformed = df_with_region\n",
    "\n",
    "df_transformed = df_transformed \\\n",
    "    .withColumn(\"Avg_Price\", (col(\"Min_Price\") + col(\"Max_Price\")) / 2)\n",
    "\n",
    "df_transformed = df_transformed \\\n",
    "    .withColumn(\"Price_Volatility_Pct\", \n",
    "                round(((col(\"Max_Price\") - col(\"Min_Price\")) / col(\"Modal_Price\")) * 100, 2))\n",
    "\n",
    "# Show sample\n",
    "df_transformed.select(\n",
    "    \"Commodity\", \"Min_Price\", \"Max_Price\", \"Modal_Price\", \n",
    "    \"Avg_Price\", \"Price_Volatility_Pct\"\n",
    ").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fa1f83e-0677-453b-9b3c-fd5bbb1ab8e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL QUERY 1: Top 10 Most Volatile Commodities by Region\n+-------+-----------------------+--------------+------------+---------+\n|Region |Commodity              |Avg_Volatility|Record_Count|Avg_Price|\n+-------+-----------------------+--------------+------------+---------+\n|West   |Brinjal                |210.48        |71133       |3545.68  |\n|North  |Mace                   |128.0         |1           |3400.0   |\n|Central|Garlic                 |124.48        |34784       |6824.65  |\n|West   |Dry Grapes             |124.29        |236         |14531.36 |\n|West   |She Buffalo            |115.69        |461         |46369.82 |\n|Central|Ashwagandha            |115.57        |425         |20076.24 |\n|West   |Cow                    |112.46        |484         |43931.49 |\n|North  |Custard Apple (Sharifa)|109.84        |224         |7824.35  |\n|West   |Goat                   |108.26        |341         |7443.78  |\n|East   |Goat                   |108.12        |600         |9672.17  |\n+-------+-----------------------+--------------+------------+---------+\n\n\nSQL QUERY 2: Price Comparison - Above vs Below Modal Price by State\n+----------------+-----------------+-----------------+---------------------+---------------------+-------------+\n|State           |Above_Modal_Count|Below_Modal_Count|Avg_Price_Above_Modal|Avg_Price_Below_Modal|Total_Records|\n+----------------+-----------------+-----------------+---------------------+---------------------+-------------+\n|Uttar Pradesh   |913328           |3050355          |3478.74              |3400.31              |3963683      |\n|Kerala          |443189           |1692055          |5569.28              |6000.5               |2135244      |\n|Madhya Pradesh  |337782           |1083709          |3308.54              |3636.84              |1421491      |\n|Maharashtra     |253890           |1192563          |4179.75              |5698.87              |1446453      |\n|Punjab          |249677           |903986           |3012.06              |2797.82              |1153663      |\n|Haryana         |207380           |690600           |2606.74              |2757.14              |897980       |\n|Gujarat         |138379           |805199           |5042.16              |4838.06              |943578       |\n|Rajasthan       |138092           |663317           |4346.6               |4079.59              |801409       |\n|Odisha          |120421           |293402           |3931.07              |3670.5               |413823       |\n|West Bengal     |120287           |590830           |4397.74              |3947.58              |711117       |\n|Karnataka       |110770           |486097           |7229.62              |6473.27              |596867       |\n|Himachal Pradesh|106720           |408579           |4478.46              |4959.34              |515299       |\n|Telangana       |69916            |437286           |3828.7               |3314.49              |507202       |\n|Uttrakhand      |56324            |182633           |2214.56              |1935.07              |238957       |\n|Chattisgarh     |34212            |204457           |2967.79              |2695.49              |238669       |\n+----------------+-----------------+-----------------+---------------------+---------------------+-------------+\n\n\n"
     ]
    }
   ],
   "source": [
    "# sql queries\n",
    "\n",
    "df_transformed.createOrReplaceTempView(\"df_transformed\")\n",
    "\n",
    "print(\"SQL QUERY 1: Top 10 Most Volatile Commodities by Region\")\n",
    "\n",
    "query1 = \"\"\"\n",
    "SELECT \n",
    "    Region,\n",
    "    Commodity,\n",
    "    ROUND(AVG(Price_Volatility_Pct), 2) as Avg_Volatility,\n",
    "    COUNT(*) as Record_Count,\n",
    "    ROUND(AVG(Avg_Price), 2) as Avg_Price\n",
    "FROM df_transformed\n",
    "WHERE Price_Volatility_Pct IS NOT NULL\n",
    "GROUP BY Region, Commodity\n",
    "ORDER BY Avg_Volatility DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "result1 = spark.sql(query1)\n",
    "result1.show(10, truncate=False)\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"SQL QUERY 2: Price Comparison - Above vs Below Modal Price by State\")\n",
    "\n",
    "\n",
    "query2 = \"\"\"\n",
    "SELECT \n",
    "    State,\n",
    "    COUNT(CASE WHEN Avg_Price > Modal_Price THEN 1 END) as Above_Modal_Count,\n",
    "    COUNT(CASE WHEN Avg_Price <= Modal_Price THEN 1 END) as Below_Modal_Count,\n",
    "    ROUND(AVG(CASE WHEN Avg_Price > Modal_Price THEN Avg_Price END), 2) as Avg_Price_Above_Modal,\n",
    "    ROUND(AVG(CASE WHEN Avg_Price <= Modal_Price THEN Avg_Price END), 2) as Avg_Price_Below_Modal,\n",
    "    COUNT(*) as Total_Records\n",
    "FROM df_transformed\n",
    "WHERE Avg_Price IS NOT NULL AND Modal_Price IS NOT NULL\n",
    "GROUP BY State\n",
    "HAVING COUNT(*) > 100\n",
    "ORDER BY Above_Modal_Count DESC\n",
    "LIMIT 15\n",
    "\"\"\"\n",
    "\n",
    "result2 = spark.sql(query2)\n",
    "result2.show(15, truncate=False)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5584a70f-4220-472f-8a11-46540c9a490a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformed data to Parquet table...\nResults written to table: commodity_analysis_results\n\nVerifying saved data:\n   • Rows saved: 20,059,718\n\n+--------------+-------------+-------------+--------------------+-----------+------+------------+---------+---------+-----------+--------------+-------+----------------+---------+--------------------+\n|         State|     District|       Market|           Commodity|    Variety| Grade|Arrival_Date|Min_Price|Max_Price|Modal_Price|Commodity_Code| Region|Region_Full_Name|Avg_Price|Price_Volatility_Pct|\n+--------------+-------------+-------------+--------------------+-----------+------+------------+---------+---------+-----------+--------------+-------+----------------+---------+--------------------+\n|Madhya Pradesh|      Khandwa|Khandwa (F&V)|        Sponge gourd|      Other|   FAQ|  2023-08-27|   1200.0|   3500.0|     2545.0|           311|Central|  Central Region|   2350.0|               90.37|\n|   Maharashtra|         Pune|         Pune|              Kakada|      Other|   FAQ|  2023-08-27|  30000.0|  40000.0|    35000.0|           230|   West|  Western Region|  35000.0|               28.57|\n| Uttar Pradesh|       Badaun|    Wazirganj|         Pomegranate|Pomogranate|Medium|  2023-08-27|   6380.0|   6480.0|     6430.0|           190|  North| Northern Region|   6430.0|                1.56|\n| Uttar Pradesh| Bulandshahar| Jahangirabad|               Apple|  Delicious|Medium|  2023-08-27|   5345.0|   5555.0|     5450.0|            17|  North| Northern Region|   5450.0|                3.85|\n|    Uttrakhand|     Haridwar|     Manglaur|             Pumpkin|      Other|   FAQ|  2023-08-27|   1000.0|   1200.0|     1100.0|            84|  North| Northern Region|   1100.0|               18.18|\n|   West Bengal|   Jalpaiguri|     Belacoba|               Onion|      Other|   FAQ|  2023-08-27|   2800.0|   3000.0|     2900.0|            23|   East|  Eastern Region|   2900.0|                 6.9|\n|   West Bengal|   Jalpaiguri|    Moynaguri|       Sweet Pumpkin|      Other|   FAQ|  2023-08-27|   1900.0|   2100.0|     2000.0|           173|   East|  Eastern Region|   2000.0|                10.0|\n|       Gujarat|    Bhavnagar|       Taleja|Sesamum (Sesame,G...|      White|   FAQ|  2023-08-28|  12500.0|  15700.0|    14100.0|            11|   West|  Western Region|  14100.0|                22.7|\n|       Gujarat|Surendranagar|      Vadhvan|         Onion Green|Onion Green|   FAQ|  2023-08-28|   2500.0|   4000.0|     3250.0|           358|   West|  Western Region|   3250.0|               46.15|\n|       Haryana|       Hissar|        Hansi|Bhindi (Ladies Fi...|      Other|   FAQ|  2023-08-28|    800.0|   1000.0|      900.0|            85|  North| Northern Region|    900.0|               22.22|\n+--------------+-------------+-------------+--------------------+-----------+------+------------+---------+---------+-----------+--------------+-------+----------------+---------+--------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Write the transformed data to a Parquet table\n",
    "print(\"Writing transformed data to Parquet table...\")\n",
    "df_transformed.write.mode(\"overwrite\").saveAsTable(\"commodity_analysis_results\")\n",
    "\n",
    "print(\"Results written to table: commodity_analysis_results\")\n",
    "print()\n",
    "\n",
    "# Verify the write\n",
    "print(\"Verifying saved data:\")\n",
    "saved_df = spark.sql(\"SELECT * FROM commodity_analysis_results\")\n",
    "print(f\"   • Rows saved: {saved_df.count():,}\")\n",
    "print()\n",
    "\n",
    "saved_df.show(10)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb180330-9595-4d02-8ab3-6fe1bef596e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution plan for the aggregation query:\n== Parsed Logical Plan ==\n'GlobalLimit 10\n+- 'LocalLimit 10\n   +- 'Sort ['Avg_Volatility DESC NULLS LAST], true\n      +- 'Aggregate ['Region, 'Commodity], ['Region, 'Commodity, 'ROUND('AVG('Price_Volatility_Pct), 2) AS Avg_Volatility#17878, 'COUNT(1) AS Record_Count#17879, 'ROUND('AVG('Avg_Price), 2) AS Avg_Price#17880]\n         +- 'Filter isnotnull('Price_Volatility_Pct)\n            +- 'UnresolvedRelation [df_transformed], [], false\n\n== Analyzed Logical Plan ==\nRegion: string, Commodity: string, Avg_Volatility: double, Record_Count: bigint, Avg_Price: double\nGlobalLimit 10\n+- LocalLimit 10\n   +- Sort [Avg_Volatility#17878 DESC NULLS LAST], true\n      +- Aggregate [Region#16594, Commodity#16236], [Region#16594, Commodity#16236, round(avg(Price_Volatility_Pct#16599), 2) AS Avg_Volatility#17878, count(1) AS Record_Count#17879L, round(avg(Avg_Price#16597), 2) AS Avg_Price#17880]\n         +- Filter isnotnull(Price_Volatility_Pct#16599)\n            +- SubqueryAlias df_transformed\n               +- View (`df_transformed`, [State#16233, District#16234, Market#16235, Commodity#16236, Variety#16237, Grade#16238, Arrival_Date#16239, Min_Price#16240, Max_Price#16241, Modal_Price#16242, Commodity_Code#16243L, Region#16594, Region_Full_Name#16595, Avg_Price#16597, Price_Volatility_Pct#16599])\n                  +- Project [State#16233, District#16234, Market#16235, Commodity#16236, Variety#16237, Grade#16238, Arrival_Date#16239, Min_Price#16240, Max_Price#16241, Modal_Price#16242, Commodity_Code#16243L, Region#16594, Region_Full_Name#16595, Avg_Price#16597, round((((Max_Price#16241 - Min_Price#16240) / Modal_Price#16242) * cast(100 as double)), 2) AS Price_Volatility_Pct#16599]\n                     +- Project [State#16233, District#16234, Market#16235, Commodity#16236, Variety#16237, Grade#16238, Arrival_Date#16239, Min_Price#16240, Max_Price#16241, Modal_Price#16242, Commodity_Code#16243L, Region#16594, Region_Full_Name#16595, ((Min_Price#16240 + Max_Price#16241) / cast(2 as double)) AS Avg_Price#16597]\n                        +- Project [State#16233, District#16234, Market#16235, Commodity#16236, Variety#16237, Grade#16238, Arrival_Date#16239, Min_Price#16240, Max_Price#16241, Modal_Price#16242, Commodity_Code#16243L, Region#16594, Region_Full_Name#16595]\n                           +- Join LeftOuter, (State#16233 = State#16593)\n                              :- Deduplicate [State#16233, District#16234, Market#16235, Commodity#16236, Variety#16237, Grade#16238, Arrival_Date#16239, Min_Price#16240, Max_Price#16241, Modal_Price#16242, Commodity_Code#16243L]\n                              :  +- Filter (((((isnotnull(Min_Price#16240) AND isnotnull(Max_Price#16241)) AND isnotnull(Modal_Price#16242)) AND (Min_Price#16240 > cast(0 as double))) AND (Max_Price#16241 > cast(0 as double))) AND (Modal_Price#16242 > cast(0 as double)))\n                              :     +- SubqueryAlias workspace.default.commodities_india\n                              :        +- Relation workspace.default.commodities_india[State#16233,District#16234,Market#16235,Commodity#16236,Variety#16237,Grade#16238,Arrival_Date#16239,Min_Price#16240,Max_Price#16241,Modal_Price#16242,Commodity_Code#16243L] parquet\n                              +- Project [State#16215 AS State#16593, Region#16216 AS Region#16594, Region_Full_Name#16217 AS Region_Full_Name#16595]\n                                 +- LocalRelation [State#16215, Region#16216, Region_Full_Name#16217]\n\n== Optimized Logical Plan ==\nGlobalLimit 10\n+- LocalLimit 10\n   +- Sort [Avg_Volatility#17878 DESC NULLS LAST], true\n      +- Aggregate [Region#16594, Commodity#16236], [Region#16594, Commodity#16236, round(avg(Price_Volatility_Pct#16599), 2) AS Avg_Volatility#17878, count(1) AS Record_Count#17879L, round(avg(Avg_Price#16597), 2) AS Avg_Price#17880]\n         +- Project [Commodity#16236, Region#16594, ((Min_Price#16240 + Max_Price#16241) / 2.0) AS Avg_Price#16597, round((((Max_Price#16241 - Min_Price#16240) / Modal_Price#16242) * 100.0), 2) AS Price_Volatility_Pct#16599]\n            +- Join LeftOuter, (State#16233 = State#16593)\n               :- Aggregate [State#16233, District#16234, Market#16235, Commodity#16236, Variety#16237, Grade#16238, Arrival_Date#16239, Min_Price#16240, Max_Price#16241, Modal_Price#16242, Commodity_Code#16243L], [State#16233, Commodity#16236, Min_Price#16240, Max_Price#16241, Modal_Price#16242]\n               :  +- Filter ((((((isnotnull(Min_Price#16240) AND isnotnull(Max_Price#16241)) AND isnotnull(Modal_Price#16242)) AND (Min_Price#16240 > 0.0)) AND (Max_Price#16241 > 0.0)) AND (Modal_Price#16242 > 0.0)) AND isnotnull(round((((Max_Price#16241 - Min_Price#16240) / Modal_Price#16242) * 100.0), 2)))\n               :     +- Relation workspace.default.commodities_india[State#16233,District#16234,Market#16235,Commodity#16236,Variety#16237,Grade#16238,Arrival_Date#16239,Min_Price#16240,Max_Price#16241,Modal_Price#16242,Commodity_Code#16243L] parquet\n               +- Filter isnotnull(State#16593)\n                  +- LocalRelation [State#16593, Region#16594]\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonTopK(limit=10, includeTies=false, sortOrder=[Avg_Volatility#17878 DESC NULLS LAST], partitionOrderCount=0)\n         +- PhotonShuffleExchangeSource\n            +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#19810]\n               +- PhotonShuffleExchangeSink SinglePartition\n                  +- PhotonTopK(limit=10, includeTies=false, sortOrder=[Avg_Volatility#17878 DESC NULLS LAST], partitionOrderCount=0)\n                     +- PhotonGroupingAgg(limit=None, keys=[Region#16594, Commodity#16236], functions=[avg(Price_Volatility_Pct#16599), count(1), avg(Avg_Price#16597)], output=[Region#16594, Commodity#16236, Avg_Volatility#17878, Record_Count#17879L, Avg_Price#17880])\n                        +- PhotonProject [Commodity#16236, Region#16594, ((Min_Price#16240 + Max_Price#16241) / 2.0) AS Avg_Price#16597, round((((Max_Price#16241 - Min_Price#16240) / Modal_Price#16242) * 100.0), 2) AS Price_Volatility_Pct#16599]\n                           +- PhotonBroadcastHashJoin [State#16233], [State#16593], LeftOuter, BuildRight, false, true\n                              :- PhotonGroupingAgg(limit=None, keys=[State#16233, District#16234, Market#16235, Commodity#16236, Variety#16237, Grade#16238, Arrival_Date#16239, Min_Price#16240, Max_Price#16241, Modal_Price#16242, Commodity_Code#16243L], functions=[], output=[State#16233, Commodity#16236, Min_Price#16240, Max_Price#16241, Modal_Price#16242])\n                              :  +- PhotonShuffleExchangeSource\n                              :     +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#19789]\n                              :        +- PhotonShuffleExchangeSink hashpartitioning(Commodity#16236, 1024)\n                              :           +- PhotonGroupingAgg(limit=None, keys=[State#16233, District#16234, Market#16235, Commodity#16236, Variety#16237, Grade#16238, Arrival_Date#16239, knownfloatingpointnormalized(normalizenanandzero(Min_Price#16240)) AS Min_Price#16240, knownfloatingpointnormalized(normalizenanandzero(Max_Price#16241)) AS Max_Price#16241, knownfloatingpointnormalized(normalizenanandzero(Modal_Price#16242)) AS Modal_Price#16242, Commodity_Code#16243L], functions=[], output=[State#16233, District#16234, Market#16235, Commodity#16236, Variety#16237, Grade#16238, Arrival_Date#16239, Min_Price#16240, Max_Price#16241, Modal_Price#16242, Commodity_Code#16243L])\n                              :              +- PhotonScan parquet workspace.default.commodities_india[State#16233,District#16234,Market#16235,Commodity#16236,Variety#16237,Grade#16238,Arrival_Date#16239,Min_Price#16240,Max_Price#16241,Modal_Price#16242,Commodity_Code#16243L] DataFilters: [isnotnull(Min_Price#16240), isnotnull(Max_Price#16241), isnotnull(Modal_Price#16242), (Min_Price..., DictionaryFilters: [(Min_Price#16240 > 0.0), (Modal_Price#16242 > 0.0), (Max_Price#16241 > 0.0)], Format: parquet, Location: PreparedDeltaFileIndex(1 paths)[s3://dbstorage-prod-x5042/uc/02476681-8374-43b2-bf7f-16ac00849806..., OptionalDataFilters: [], PartitionFilters: [], ReadSchema: struct<State:string,District:string,Market:string,Commodity:string,Variety:string,Grade:string,Ar..., RequiredDataFilters: [isnotnull(Min_Price#16240), isnotnull(Max_Price#16241), isnotnull(Modal_Price#16242), (Min_Price...\n                              +- PhotonShuffleExchangeSource\n                                 +- PhotonShuffleMapStage EXECUTOR_BROADCAST, [id=#19799]\n                                    +- PhotonShuffleExchangeSink SinglePartition\n                                       +- PhotonFilter isnotnull(State#16593)\n                                          +- PhotonRowToColumnar\n                                             +- LocalTableScan [State#16593, Region#16594]\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n== Optimizer Statistics (table names per statistics state) ==\n  missing = \n  partial = \n  full    = commodities_india\n\n"
     ]
    }
   ],
   "source": [
    "print(\"Execution plan for the aggregation query:\")\n",
    "result1.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c84942-49a6-460e-9a51-1c34fcdb71c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------------------+---------+---------+-----------+\n|         State|       Commodity|              Market|Min_Price|Max_Price|Modal_Price|\n+--------------+----------------+--------------------+---------+---------+-----------+\n|Andhra Pradesh|   Gur (Jaggery)|            Chittoor|   3200.0|   3500.0|     3500.0|\n|    Tamil Nadu|          Garlic|Gudalur (Uzhavar ...|  28000.0|  30000.0|    30000.0|\n|    Tamil Nadu|  Ginger (Green)|Gudalur (Uzhavar ...|   7000.0|   8000.0|     8000.0|\n|    Tamil Nadu|    Green Chilli|Gudalur (Uzhavar ...|   4500.0|   5000.0|     5000.0|\n|    Tamil Nadu|   Mint (Pudina)|Gudalur (Uzhavar ...|   4500.0|   5000.0|     5000.0|\n|    Tamil Nadu|           Onion|Gudalur (Uzhavar ...|   4200.0|   4400.0|     4400.0|\n|    Tamil Nadu|     Onion Green|Gudalur (Uzhavar ...|   7000.0|   7400.0|     7400.0|\n|    Tamil Nadu|          Potato|Gudalur (Uzhavar ...|   4500.0|   5000.0|     5000.0|\n|    Tamil Nadu|         Pumpkin|Gudalur (Uzhavar ...|   1800.0|   2000.0|     2000.0|\n|    Tamil Nadu|         Raddish|Gudalur (Uzhavar ...|   3000.0|   3500.0|     3500.0|\n|    Tamil Nadu|      Snakeguard|Gudalur (Uzhavar ...|   3500.0|   4000.0|     4000.0|\n|    Tamil Nadu|         Tapioca|Gudalur (Uzhavar ...|   4000.0|   4500.0|     4500.0|\n|    Tamil Nadu|      Snakeguard|Thalavaipuram (Uz...|   4000.0|   4500.0|     4500.0|\n|    Tamil Nadu|          Potato|Thalavaipuram (Uz...|   4500.0|   5000.0|     5000.0|\n|    Tamil Nadu|Amla (Nelli Kai)|Udhagamandalam (U...|   6500.0|   7000.0|     7000.0|\n|    Tamil Nadu|           Apple|Udhagamandalam (U...|  15000.0|  18000.0|    18000.0|\n|    Tamil Nadu|        Ashgourd|Udhagamandalam (U...|   1500.0|   2000.0|     2000.0|\n|    Tamil Nadu|          Banana|Udhagamandalam (U...|   3500.0|   5000.0|     5000.0|\n|    Tamil Nadu|  Banana - Green|Udhagamandalam (U...|   3500.0|   4000.0|     4000.0|\n|    Tamil Nadu|           Beans|Udhagamandalam (U...|   7000.0|   8000.0|     8000.0|\n+--------------+----------------+--------------------+---------+---------+-----------+\nonly showing top 20 rows\nTransformation Time: 0.0003 seconds\nRecord Count: 20,090,620\nAction (Show) Time: 0.4267 seconds\n"
     ]
    }
   ],
   "source": [
    "# Actions vs. Transformations\n",
    "\n",
    "import time\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = spark.table(\"commodities_india\")\n",
    "\n",
    "# Transformation: Select specific columns (lazy)\n",
    "start_transformation_time = time.time()\n",
    "transformation = df.select(\"State\", \"Commodity\", \"Market\", \"Min_Price\", \"Max_Price\", \"Modal_Price\")\n",
    "end_transformation_time = time.time()\n",
    "transformation_time = end_transformation_time - start_transformation_time\n",
    "\n",
    "# Action: Count the number of records (eager)\n",
    "start_action_count_time = time.time()\n",
    "record_count = transformation.count()\n",
    "end_action_count_time = time.time()\n",
    "action_count_time = end_action_count_time - start_action_count_time\n",
    "\n",
    "# Action: Show the first few rows (eager)\n",
    "start_action_show_time = time.time()\n",
    "sample_data = transformation.show()\n",
    "end_action_show_time = time.time()\n",
    "action_show_time = end_action_show_time - start_action_show_time\n",
    "\n",
    "# results\n",
    "print(f\"Transformation Time: {transformation_time:.4f} seconds\")\n",
    "print(f\"Record Count: {record_count:,}\")\n",
    "print(f\"Action (Show) Time: {action_show_time:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Week 11: PySpark Data Processing Assignment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}